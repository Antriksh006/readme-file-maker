{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "g = Github(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = g.get_repo(\"Antriksh006/dav-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping binary file: .gitignore\n",
      "Skipping binary file: README.md\n",
      "Skipping binary file: feature_comparison.png\n",
      "Skipping binary file: image_features.csv\n",
      "Skipping binary file: pca_visualization.png\n",
      "Skipping binary file: tsne_visualization.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "from github import Github  # Assuming you're using PyGithub\n",
    "\n",
    "# Define output folder\n",
    "output_folder = \"Readme Maker\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Function to convert Jupyter notebook to Python script\n",
    "def convert_notebook_to_script(notebook_content):\n",
    "    \"\"\"Converts Jupyter notebook content to a Python script.\"\"\"\n",
    "    notebook_node = nbformat.reads(notebook_content, as_version=4)\n",
    "    exporter = PythonExporter()\n",
    "    script_content, _ = exporter.from_notebook_node(notebook_node)\n",
    "    return script_content\n",
    "\n",
    "contents = repo.get_contents(\"\")  # Get all repository contents\n",
    "\n",
    "# Process files in the repository\n",
    "while contents:\n",
    "    file_content = contents.pop(0)\n",
    "\n",
    "    if file_content.type == \"dir\":\n",
    "        contents.extend(repo.get_contents(file_content.path))  # Recursively process subdirectories\n",
    "\n",
    "    else:\n",
    "        file_extension = os.path.splitext(file_content.path)[1]\n",
    "\n",
    "        if file_extension == '.ipynb':\n",
    "            # Convert Jupyter notebook to Python script\n",
    "            notebook_content = file_content.decoded_content.decode(\"utf-8\")\n",
    "            python_script = convert_notebook_to_script(notebook_content)\n",
    "\n",
    "            python_file_path = os.path.join(output_folder, os.path.basename(file_content.path).replace('.ipynb', '.py'))\n",
    "            \n",
    "            # Save the converted Python script\n",
    "            with open(python_file_path, 'w', encoding='utf-8') as py_file:\n",
    "                py_file.write(python_script)\n",
    "\n",
    "            # Also save as a text file\n",
    "            text_file_path = python_file_path.replace('.py', '.txt')\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(python_script)\n",
    "\n",
    "        elif file_extension == '.py':\n",
    "            # Extract text from an existing Python file\n",
    "            python_file_content = file_content.decoded_content.decode(\"utf-8\")\n",
    "            text_file_path = os.path.join(output_folder, os.path.basename(file_content.path).replace('.py', '.txt'))\n",
    "\n",
    "            # Save Python code to a text file\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(python_file_content)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping binary file: {file_content.path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_response(text, groq_client):\n",
    "    detailed_prompt = f\"I am making a github readme file maker, in that i will provide you with code of a file, and you have to write a 100-200 words description for that code. Here is the code of the file: {text}, Make sure you cover any module packages used, and the purpose of the code. i will return all of the descriptions in a file and then pass all the that code to inference again, to make the final readme file. You just focus on the first part. Make sure you dont answer in more than 200-300 words. Also add snippets of code to explain the code better.\"\n",
    "\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": detailed_prompt}\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for 3_ai.txt:\n",
      "Here is a 100-200 word description for the code:\n",
      "\n",
      "This Python script, Response for 3.txt, is designed to explore and preprocess two datasets of medical images: DICOM images and grayscale images. The script utilizes the `pydicom` and `PIL` libraries to read and manipulate the images. The code defines two functions, `explore_dicom_data` and `explore_image_data`, to examine the metadata and content of the images. The metadata includes information such as the image's modality, manufacturer, and SOP Instance UID. The script then preprocesses the images by converting them to numpy arrays and normalizing the pixel values.\n",
      "\n",
      "Here is an example of how the script reads and converts a DICOM image:\n",
      "\n",
      "```\n",
      "import pydicom\n",
      "ds = pydicom.dcmread('image.dcm')\n",
      "array = ds['pixel_array']\n",
      "```\n",
      "\n",
      "In this snippet, the `pydicom` library is used to read a DICOM image and extract its pixel array.\n",
      "\n",
      "Response for 3.txt:\n",
      "Here is the description for the provided code:\n",
      "\n",
      "This Python script is designed to explore and preprocess medical image datasets. Specifically, it deals with two types of images: DICOM files and HC/PD images. The script begins by defining two functions: `explore_dicom_data` and `explore_image_data`. The former is used to extract metadata from a selection of DICOM files, while the latter examines the contents of folders containing HC and PD images.\n",
      "\n",
      "The script then preprocesses the datasets using two functions: `preprocess_dicom` and `preprocess_dataset`. The former converts DICOM files to normalized numpy arrays, while the latter processes both DICOM and image datasets, converting them to grayscale and normalizing them.\n",
      "\n",
      "The script also defines a function called `extract_image_features` to extract statistical and textural features from images using Gray-Level Co-Occurrence Matrix (GLCM) and Shannon entropy. These features are used to create a Pandas DataFrame, which is further analyzed for feature distribution, dimensionality reduction, and clustering.\n",
      "\n",
      "The script concludes by creating interactive visualizations and a dashboard using Plotly and Dash frameworks, allowing for exploratory data analysis and visualization of the extracted features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "API_KEY =  (\"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\")\n",
    "    # Initialize Groq client\n",
    "groq_client = Groq(api_key=API_KEY)\n",
    "folder_path = r\"Readme Maker\"\n",
    "\n",
    "while True:\n",
    "    text_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    \n",
    "    for text_file in text_files:\n",
    "        text_file_path = os.path.join(folder_path, text_file)\n",
    "        \n",
    "        with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "        \n",
    "        response = get_bot_response(text_content, groq_client)\n",
    "        print(f\"Response for {text_file}:\\n{response}\\n\")\n",
    "    text_file_path = text_file.replace('.txt', '')\n",
    "    ai_text_file_path = os.path.join(folder_path, f\"{text_file_path}_ai.txt\")\n",
    "    with open(ai_text_file_path, 'a', encoding='utf-8') as ai_file:\n",
    "        ai_file.write(f\"Response for {text_file}:\\n{response}\\n\\n\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_response_readme(text, groq_client):\n",
    "    detailed_prompt = f\"I am giving you brief description about the code of every file i have written to make a project, i want you to create a github readme.md file using all this information about the code. Here is the code of the file: {text}. Make sure to include code snippets and explain the code in a way that anyone can understand. Make sure the readme is complete, with the code explanation, installation instructions, and usage instructions. You can also add any additional information you think is relevant.\"\n",
    "\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": detailed_prompt}\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the complete README.md file for your project:\n",
      "\n",
      "**Medical Image Processing and Analysis**\n",
      "\n",
      "This repository contains a Python script that explores and preprocesses two datasets of medical images: DICOM images and grayscale images. The script uses various libraries such as `pydicom`, `PIL`, `scipy`, `pandas`, and `plotly` to read, manipulate, and analyze the images.\n",
      "\n",
      "**Installation**\n",
      "\n",
      "To run this script, you'll need to have Python installed on your machine. You'll also need to install the following libraries:\n",
      "\n",
      "* `pydicom`\n",
      "* `PIL`\n",
      "* `scipy`\n",
      "* `pandas`\n",
      "* `plotly`\n",
      "\n",
      "You can install these libraries using pip:\n",
      "```\n",
      "pip install pydicom pillow scipy pandas plotly\n",
      "```\n",
      "**Usage**\n",
      "\n",
      "To run the script, simply execute the `main.py` file:\n",
      "```\n",
      "python main.py\n",
      "```\n",
      "The script will run through several steps:\n",
      "\n",
      "1. **Explores DICOM data**: The script uses the `explore_dicom_data` function to extract metadata from a selection of DICOM files.\n",
      "2. **Explains HC/PD image data**: The script uses the `explore_image_data` function to examine the contents of folders containing HC and PD images.\n",
      "3. **Preprocesses datasets**: The script uses the `preprocess_dicom` and `preprocess_dataset` functions to convert DICOM files to normalized numpy arrays and process both DICOM and image datasets, converting them to grayscale and normalizing them.\n",
      "4. **Extracts image features**: The script uses the `extract_image_features` function to extract statistical and textural features from images using Gray-Level Co-Occurrence Matrix (GLCM) and Shannon entropy.\n",
      "5. **Creates a Pandas DataFrame**: The script creates a Pandas DataFrame to store the features and performs dimensionality reduction using PCA and t-SNE.\n",
      "6. **Creates interactive visualizations**: The script creates interactive visualizations using Plotly and Dash frameworks, allowing for exploratory data analysis and visualization of the extracted features.\n",
      "\n",
      "**Code Explanation**\n",
      "\n",
      "Here is an example of the `explore_dicom_data` function:\n",
      "```python\n",
      "import pydicom\n",
      "\n",
      "def explore_dicom_data(dicom_files):\n",
      "    for file in dicom_files:\n",
      "        ds = pydicom.dcmread(file)\n",
      "        print(ds.PatientName)\n",
      "        print(ds.Modality)\n",
      "        print(ds.SeriesDescription)\n",
      "        # ... other metadata extraction ...\n",
      "```\n",
      "This function reads each DICOM file and extracts the patient name, modality, and series description metadata.\n",
      "\n",
      "Here is an example of the `preprocess_dicom` function:\n",
      "```python\n",
      "import numpy as np\n",
      "import scipy\n",
      "\n",
      "def preprocess_dicom(dicom_file):\n",
      "    ds = pydicom.dcmread(dicom_file)\n",
      "    img = ds.pixel_array\n",
      "    img = img.astype(np.float32)\n",
      "    img = img / np.max(img)\n",
      "    return img\n",
      "```\n",
      "This function reads a DICOM file, converts the pixel array to a numpy array, and normalizes the pixel values.\n",
      "\n",
      "**Project Structure**\n",
      "\n",
      "The project is structured as follows:\n",
      "\n",
      "* `main.py`: The main script that runs the entire analysis\n",
      "* `data`: The folder containing the medical image datasets\n",
      "* `utils`: The folder containing utility functions and libraries\n",
      "* `reports`: The folder containing the intermediate results and output files\n",
      "\n",
      "I hope this README.md file provides a clear and concise explanation of the code and project structure. Let me know if you have any questions or if there's anything else I can help with!\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path\n",
    "folder_path = \"Readme Maker\"\n",
    "\n",
    "# Initialize an empty string to store all the text\n",
    "all_text = \"\"\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('_ai.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            all_text += file.read() + \"\\n\"\n",
    "\n",
    "# Pass the concatenated text to get_bot_response_readme\n",
    "final_response = get_bot_response_readme(all_text, groq_client)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
